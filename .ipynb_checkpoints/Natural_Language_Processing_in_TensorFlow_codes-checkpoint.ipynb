{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# course information\n",
    "# Coursera, Natural Language Processing in TensorFlow, deeplearning.ai,\n",
    "\n",
    "# https://www.coursera.org/learn/natural-language-processing-tensorflow#about\n",
    "\n",
    "# Week 1: Tokenization, ASCII character codes have some problem during word encoding(e.g. LISTEN and SILENT). \n",
    "# Week2: Word Embedding. \n",
    "# Week3: Sequence models.\n",
    "# Week4: Sequence models and literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n",
      "**********\n",
      "[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n",
      "**********\n",
      "[[4, 1, 2, 3, 5], [3, 5, 1, 3, 1], [1, 7, 1, 3, 5, 1, 1]]\n",
      "**********\n",
      "[[0 0 4 1 2 3 5]\n",
      " [0 0 3 5 1 3 1]\n",
      " [1 7 1 3 5 1 1]]\n",
      "**********\n",
      "[[4 1 2 3 5]\n",
      " [3 5 1 3 1]\n",
      " [1 3 5 1 1]]\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!'\n",
    "] #punctuation does not impact tokenization(clean up), not case sensitivity\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>') \n",
    "# num_words is not necessary\n",
    "#do not ignore untokenized words\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index #key-value pairs, key is the word and value is the token of the word\n",
    "print(word_index)\n",
    "print(\"*\"*10)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)\n",
    "print(\"*\"*10)\n",
    "\n",
    "test_data = [\n",
    "    'I really love my dog',\n",
    "    'My dog loves my manatee',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)\n",
    "print(\"*\"*10)\n",
    "\n",
    "padded = pad_sequences(test_seq)\n",
    "print(padded)\n",
    "print(\"*\"*10)\n",
    "\n",
    "padded = pad_sequences(test_seq, padding='post', maxlen=5) #pre more words are lost\n",
    "print(padded)\n",
    "print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install -q tensorflow-datasets\n",
    "#IMDB sentiment classification\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(str(s.tonumpy()))\n",
    "    training_labels.append(l.tonumpy())\n",
    "    \n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(str(s.tonumpy()))\n",
    "    testing_labels.append(l.tonumpy())\n",
    "    \n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok) #out of vocabulary token\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length,truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
    "\n",
    "# model structure\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "num_epochs = 10\n",
    "model.fit(padded,\n",
    "         training_labels_final,\n",
    "          epochs=num_epochs,\n",
    "          validation_data=(testing_padded,testing_labels_final))\n",
    "\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) #shape: (vocab_size, embedding_dim)\n",
    "\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "#tensorboard word embedding visualization files\n",
    "import io\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + '\\n')\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings])+'\\n')\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('vecs.tsv')\n",
    "    files.download('meta.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subword tokenization\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews/subwords8k\". with_info=True, as_supervised=True)\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "tokenizer = info.features['text'].encoder\n",
    "print(tokenizer.subwords)\n",
    "\n",
    "sample_string = 'TensorFlow, from basics to mastery'\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('The original string:{}'.format(original_string))\n",
    "\n",
    "for ts in tokenized_string:\n",
    "    print('{}---->{}'.format(ts,tokenizer.decode([ts])))\n",
    "\n",
    "embedding_dim = 64\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6,activation='relu')\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "num_epochs = 10\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_data, epochs=num_epochs, validation_data=test_data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string,'val_'+string])\n",
    "    plt.show()\n",
    "\n",
    "plot_graph(history, 'accuracy')\n",
    "plot_graph(history, 'loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layers LSTM\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)) #hyper-parameter means dimensionality of the output space\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D convolution layers for NLP\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(128,5,activation='relu'), #128 kinds of fliters and each for 5 words\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(24,activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyric prediction\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = \"In the town of Athy one Jeremy Lanigan \\n Battered away ... ...\"\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1 # 1 for out of vocabulary\n",
    "\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "# last one is the label and previous ones are the input\n",
    "xs = input_sequences[:,:-1]\n",
    "labels = input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words) #one-hot all labels\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=500, verbose=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history,'acc')\n",
    "\n",
    "seed_text = \"Laurence went to dublin\"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
